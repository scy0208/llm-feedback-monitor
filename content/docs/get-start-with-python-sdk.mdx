---
title: Get Started
description: How to integrate python llm-feedback SDK
---

# Introduction
LLM-feedback(https://www.llmfeedback.com/) is a tool designed for LLM app developers to analyzing User-AI interaction the :

- Detect and summarize user queries that resulted in poor AI responses, potentially caused by issues with prompts, models, or RAG indexing.
- Monitor feedback on AI-generated content, staying informed about how users are impacted by changes in the LLM configuration, be it prompts, models, or RAG indexing.
- Track every modification to your LLM configuration and aggregate feedback for each version, enabling efficient LLM A/B testing.
- Gain deeper insights into user-AI interactions and the context surrounding specific feedback.


With the help of the client SDK, integration will be easy. Client SDK: https://github.com/scy0208/llm-feedback-client-py 

# Getting Started (Cloud Host Solution)
We will use Langchain team's example code chat-langchain(https://github.com/langchain-ai/chat-langchain) as an example. The detail code change can be found at (https://github.com/langchain-ai/chat-langchain/pull/129/files)

## Register and create project
Go to https://www.llmfeedback.com/register, connect your github account, and create a new project in the dashboard

## Install llm-feedback-client Python SDK
````mdx
pip install llm-feedback-client

````
## Create a client in your app:
```python showLineNumbers {3}
from llmfeedback.client import Client as FeedbackClient

project_id = "YOUR_PROJECT_ID"
api_key = "YOUR_API_KEY"
feedback_client = FeedbackClient(project_id=project_id, api_key=api_key)
```

## Register your LLM config:
For example, we have a complex system prompt, customized RAG index setting, 
and other LLM model settings. We aim to evaluate the performance of these 
configurations in production and gather the user-AI interaction data on them. 
We can combine and label these settings as a version.

```python showLineNumbers
# The System Prompt
system_message = SystemMessage(
        content=(
            "You are an expert developer tasked answering questions about the LangChain Python package. "
            "You have access to a LangChain knowledge bank which you can query but know NOTHING about LangChain otherwise. "
            "You should always first query the knowledge bank for information on the concepts in the question. "
            "For example, given the following input question:\n"
            "-----START OF EXAMPLE INPUT QUESTION-----\n"
            "What is the transform() method for runnables? \n"
            "-----END OF EXAMPLE INPUT QUESTION-----\n"
            "Your research flow should be:\n"
            "1. Query your search tool for information on 'Runnables.transform() method' to get as much context as you can about it.\n"
            "2. Then, query your search tool for information on 'Runnables' to get as much context as you can about it.\n"
            "3. Answer the question with the context you have gathered."
            "For another example, given the following input question:\n"
            "-----START OF EXAMPLE INPUT QUESTION-----\n"
            "How can I use vLLM to run my own locally hosted model? \n"
            "-----END OF EXAMPLE INPUT QUESTION-----\n"
            "Your research flow should be:\n"
            "1. Query your search tool for information on 'run vLLM locally' to get as much context as you can about it. \n"
            "2. Answer the question as you now have enough context.\n\n"
            "Include CORRECT Python code snippets in your answer if relevant to the question. If you can't find the answer, DO NOT make up an answer. Just say you don't know. "
            "Answer the following question as best you can:"
        )
    )

# The Vector Store
weaviate_client = Weaviate(
        client=client,
        index_name=WEAVIATE_DOCS_INDEX_NAME,
        text_key="text",
        embedding=OpenAIEmbeddings(chunk_size=200),
        by_text=False,
        attributes=["source"],
    )
    retriever = weaviate_client.as_retriever(
        search_kwargs=dict(k=3), callbacks=callbacks
    )

# The LLM setting
llm = ChatOpenAI(
            model="gpt-3.5-turbo-16k",
            streaming=True,
            temperature=0,
            callbacks=[QueueCallback(q)],
        )

# You can record anything critical to the performance 
# and register them together as a version of config
CONFIG_NAME = "CHAT-LANGCHAIN_09072023"
feedback_client.register_config(
        config_name=CONFIG_NAME, 
        config={
            # record type 
            "type": "OpenAIFunctionsAgent",
            # record system_prompt
            "prompt": system_message.content,
            # record llm setting
            "llm": llm._default_params,
            # record tools (including the RAG setting)
            "tools": [{
                "name": tool.name,
                "description": tool.description,
            } for tool in tools]
            # record the RAG index setting
            "rag_setting": {
                "embedding": "OpenAIEmbeddings"
                "chunk_size": 200
                "k": 3
            }
        }
    )
```

## Log the User-AI interation:
`id` can be used in link a feedback to a specific content, we will use Langchain run_id here.
`groupId` can help group together multile rounds of user-AI interations  we will use conversation_id here.
If you are handling streaming, just log the dialogue when the stream is finished
```python showLineNumbers {3}
# The loop to handle stream and accumulate tokens to the content
content = ""
while True:
    try:
        next_token = q.get(True, timeout=1)
        if next_token is job_done:
            break
        content += next_token
        yield next_token
    except Empty:
        continue

# Get run_id from Langchain callback handler
if not run_id and run_collector.traced_runs:
    run = run_collector.traced_runs[0]
    run_id = run.id

feedback_client.log_dialogue(
    config_name=CONFIG_NAME,
    instruction=question,
    response=content,
    id=str(run_id),
    group_id=data.get("conversation_id"),
    created_by="user",
)
```

## Create feedback on specific content
Assume we have a backend API handle feedback 
```python showLineNumbers {3}
@app.post("/feedback")
async def send_feedback(request: Request):
    global run_id, feedback_recorded
    data = await request.json()
    user = data.get("user")
    score = data.get("score")
    feedback_client.create_feedback(
        content_id=str(run_id), 
        key="user_score",
        score=score,
        user="user" 
    )
```
In your UI component:
```ts showLineNumbers {3}
// implementation of sendFeedback
const sendFeedback = async (user: string, score: number) => {
    setFeedback(score);
    try {
      const response = await fetch(apiBaseUrl + "/feedback", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          user: user,
          score: score,
        }),
      });
      const data = await response.json();
    } catch (e: any) {
      console.error("Error:", e);
      toast.error(e.message);
    }
  };
```
In your HTML part
```ts showLineNumbers {3}
<button type="button" onClick={() => sendFeedback(user.email, 1)}
    <ThumbUpIcon />
</button>
```
